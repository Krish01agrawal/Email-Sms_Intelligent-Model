{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸš€ Financial Email/SMS Classification with DistilBERT\n",
    "\n",
    "This notebook provides a step-by-step guide to train and test a DistilBERT model for classifying emails and SMS as financial or non-financial.\n",
    "\n",
    "## ðŸ“Š Datasets Used:\n",
    "1. `genai_gmail_chat.financial_transactions.csv`: Financial transaction data (1140 records)\n",
    "2. `pluto_money.sms_data.csv`: SMS data (83 records)\n",
    "3. `pluto_money.email_logs.csv`: Email logs (150MB)\n",
    "4. `krishplutomoney all emails gmail_data...csv`: Additional email data (117 records)\n",
    "\n",
    "## ðŸŽ¯ Goals:\n",
    "1. Classify messages as financial/non-financial\n",
    "2. Extract structured financial data\n",
    "3. Achieve >95% accuracy\n",
    "4. Store results in MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸš€ Financial Email/SMS Classification with DistilBERT\n",
    "\n",
    "This notebook provides a step-by-step guide to train and test a DistilBERT model for classifying emails and SMS as financial or non-financial.\n",
    "\n",
    "## ðŸ“Š Datasets Used:\n",
    "1. `mail_data.csv`: Base dataset with spam/ham labels\n",
    "2. `genai_gmail_chat.financial_transactions.csv`: Financial transaction data\n",
    "3. `pluto_money.sms_data.csv`: SMS data\n",
    "4. `pluto_money.email_logs.csv`: Email logs\n",
    "5. `krishplutomoney all emails gmail_data...csv`: Additional email data\n",
    "\n",
    "## ðŸŽ¯ Goals:\n",
    "1. Classify messages as financial/non-financial\n",
    "2. Extract structured financial data\n",
    "3. Achieve >95% accuracy\n",
    "4. Store results in MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdef7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add current directory to path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "\n",
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local imports\n",
    "from config import model_config, data_config\n",
    "from data_preprocessing import TextPreprocessor, DatasetPreparator\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets():\n",
    "    # Load financial transactions (these are already labeled as financial)\n",
    "    financial_data = pd.read_csv('../datasets/genai_gmail_chat.financial_transactions.csv')\n",
    "    \n",
    "    # Load SMS data\n",
    "    sms_data = pd.read_csv('../datasets/pluto_money.sms_data.csv')\n",
    "    \n",
    "    # Load email logs\n",
    "    email_logs = pd.read_csv('../datasets/pluto_money.email_logs.csv')\n",
    "    \n",
    "    # Load additional email data\n",
    "    additional_emails = pd.read_csv('../datasets/krishplutomoney all emails gmail_data_117454877979500520700_20250630_012957.csv')\n",
    "    \n",
    "    return financial_data, sms_data, email_logs, additional_emails\n",
    "\n",
    "# Load all datasets\n",
    "financial_data, sms_data, email_logs, additional_emails = load_datasets()\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Financial transactions: {len(financial_data)} records\")\n",
    "print(f\"SMS data: {len(sms_data)} records\")\n",
    "print(f\"Email logs: {len(email_logs)} records\")\n",
    "print(f\"Additional emails: {len(additional_emails)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Load and Prepare Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d775546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "def prepare_data_for_training(financial_data, sms_data, email_logs, additional_emails):\n",
    "    # 1. Process financial transactions (already labeled)\n",
    "    financial_data['text'] = financial_data['snippet']\n",
    "    financial_data['is_financial'] = 1  # All are financial\n",
    "    \n",
    "    # 2. Process SMS data\n",
    "    sms_data['text'] = sms_data['message']\n",
    "    sms_data['is_financial'] = sms_data['message'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # 3. Process email logs\n",
    "    email_logs['text'] = email_logs['subject'] + ' ' + email_logs['body']\n",
    "    email_logs['is_financial'] = email_logs['text'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # 4. Process additional emails\n",
    "    additional_emails['text'] = additional_emails['snippet']\n",
    "    additional_emails['is_financial'] = additional_emails['snippet'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # Combine all datasets\n",
    "    combined_data = pd.concat([\n",
    "        financial_data[['text', 'is_financial']],\n",
    "        sms_data[['text', 'is_financial']],\n",
    "        email_logs[['text', 'is_financial']],\n",
    "        additional_emails[['text', 'is_financial']]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Clean text\n",
    "    combined_data['text'] = combined_data['text'].apply(preprocessor.clean_text)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_data = combined_data.drop_duplicates(subset=['text'])\n",
    "    \n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(\n",
    "        combined_data, \n",
    "        test_size=0.3, \n",
    "        random_state=42,\n",
    "        stratify=combined_data['is_financial']\n",
    "    )\n",
    "    \n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=temp_df['is_financial']\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Prepare data\n",
    "train_df, val_df, test_df = prepare_data_for_training(\n",
    "    financial_data, sms_data, email_logs, additional_emails\n",
    ")\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(train_df['is_financial'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78737f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_datasets():\n",
    "    # Load mail_data.csv (spam/ham)\n",
    "    mail_data = pd.read_csv('../datasets/mail_data.csv')\n",
    "    \n",
    "    # Load financial transactions\n",
    "    financial_data = pd.read_csv('../datasets/genai_gmail_chat.financial_transactions.csv')\n",
    "    \n",
    "    # Load SMS data\n",
    "    sms_data = pd.read_csv('../datasets/pluto_money.sms_data.csv')\n",
    "    \n",
    "    # Load additional email data\n",
    "    additional_emails = pd.read_csv('../datasets/krishplutomoney all emails gmail_data_117454877979500520700_20250630_012957.csv')\n",
    "    \n",
    "    return mail_data, financial_data, sms_data, additional_emails\n",
    "\n",
    "# Load all datasets\n",
    "mail_data, financial_data, sms_data, additional_emails = load_datasets()\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Mail data: {len(mail_data)} records\")\n",
    "print(f\"Financial transactions: {len(financial_data)} records\")\n",
    "print(f\"SMS data: {len(sms_data)} records\")\n",
    "print(f\"Additional emails: {len(additional_emails)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "def prepare_data_for_training(mail_data, financial_data, sms_data, additional_emails):\n",
    "    # 1. Process mail_data (spam/ham)\n",
    "    mail_data['text'] = mail_data['Message']\n",
    "    mail_data['is_financial'] = mail_data['Category'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # 2. Process financial transactions\n",
    "    financial_data['text'] = financial_data['snippet']\n",
    "    financial_data['is_financial'] = 1  # All are financial\n",
    "    \n",
    "    # 3. Process SMS data\n",
    "    sms_data['text'] = sms_data['message']\n",
    "    sms_data['is_financial'] = sms_data['message'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # 4. Process additional emails\n",
    "    additional_emails['text'] = additional_emails['snippet']\n",
    "    additional_emails['is_financial'] = additional_emails['snippet'].apply(lambda x: \n",
    "        1 if preprocessor.extract_financial_features(x)['has_financial_indicators'] else 0\n",
    "    )\n",
    "    \n",
    "    # Combine all datasets\n",
    "    combined_data = pd.concat([\n",
    "        mail_data[['text', 'is_financial']],\n",
    "        financial_data[['text', 'is_financial']],\n",
    "        sms_data[['text', 'is_financial']],\n",
    "        additional_emails[['text', 'is_financial']]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Clean text\n",
    "    combined_data['text'] = combined_data['text'].apply(preprocessor.clean_text)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_data = combined_data.drop_duplicates(subset=['text'])\n",
    "    \n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(\n",
    "        combined_data, \n",
    "        test_size=0.3, \n",
    "        random_state=42,\n",
    "        stratify=combined_data['is_financial']\n",
    "    )\n",
    "    \n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=temp_df['is_financial']\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Prepare data\n",
    "train_df, val_df, test_df = prepare_data_for_training(\n",
    "    mail_data, financial_data, sms_data, additional_emails\n",
    ")\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(train_df['is_financial'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Initialize DistilBERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    model.to(device)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = initialize_model()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Prepare Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, tokenizer):\n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        df['text'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=model_config.max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'labels': torch.tensor(df['is_financial'].tolist())\n",
    "    }\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = prepare_dataset(train_df, tokenizer)\n",
    "val_dataset = prepare_dataset(val_df, tokenizer)\n",
    "test_dataset = prepare_dataset(test_df, tokenizer)\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Training: {train_dataset['input_ids'].shape}\")\n",
    "print(f\"Validation: {val_dataset['input_ids'].shape}\")\n",
    "print(f\"Test: {test_dataset['input_ids'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args():\n",
    "    return TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=model_config.num_epochs,\n",
    "        per_device_train_batch_size=model_config.batch_size,\n",
    "        per_device_eval_batch_size=model_config.batch_size * 2,\n",
    "        warmup_steps=model_config.warmup_steps,\n",
    "        weight_decay=model_config.weight_decay,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=model_config.logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=model_config.eval_steps,\n",
    "        save_steps=model_config.save_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=model_config.fp16 and torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=model_config.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "training_args = get_training_args()\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Epochs: {model_config.num_epochs}\")\n",
    "print(f\"Batch size: {model_config.batch_size}\")\n",
    "print(f\"Learning rate: {model_config.learning_rate}\")\n",
    "print(f\"FP16: {model_config.fp16 and torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'f1': report['weighted avg']['f1-score'],\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall']\n",
    "    }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer, test_dataset):\n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "    print(f\"Precision: {test_results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {test_results['eval_recall']:.4f}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "test_results = evaluate_model(trainer, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(trainer, test_dataset, test_df):\n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(test_df['is_financial'], preds)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Non-Financial', 'Financial'],\n",
    "        yticklabels=['Non-Financial', 'Financial']\n",
    "    )\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "\n",
    "plot_confusion_matrix(trainer, test_dataset, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Sample Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(model, tokenizer, texts):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=model_config.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'is_financial': bool(prediction.item()),\n",
    "            'confidence': probs[0][prediction.item()].item()\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Your account has been credited with Rs. 5000\",\n",
    "    \"Meeting scheduled for tomorrow at 3 PM\",\n",
    "    \"UPI payment of Rs. 2500 to Amazon completed\",\n",
    "    \"Happy birthday! Hope you have a great day\",\n",
    "    \"Your mutual fund investment of Rs. 10000 has been processed\",\n",
    "    \"Please review the attached document\",\n",
    "    \"Credit card payment due: Rs. 15000 by 15th\",\n",
    "    \"Weather forecast for today: Sunny with clear skies\"\n",
    "]\n",
    "\n",
    "results = test_predictions(model, tokenizer, test_texts)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Prediction: {'FINANCIAL' if result['is_financial'] else 'NON-FINANCIAL'}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, test_results):\n",
    "    # Create output directory\n",
    "    os.makedirs('models/distilbert', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained('models/distilbert')\n",
    "    tokenizer.save_pretrained('models/distilbert')\n",
    "    \n",
    "    # Save test results\n",
    "    import json\n",
    "    with open('models/distilbert/test_results.json', 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    \n",
    "    print(\"Model and results saved to 'models/distilbert'\")\n",
    "\n",
    "save_model(model, tokenizer, test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51232cb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "1. **Fine-tuning**: Experiment with different hyperparameters\n",
    "2. **Data Augmentation**: Add more financial examples\n",
    "3. **Error Analysis**: Review misclassified examples\n",
    "4. **Production**: Deploy model with MongoDB integration\n",
    "5. **Monitoring**: Set up performance tracking\n",
    "\n",
    "The model is now ready for:\n",
    "- Integration with your Agno framework\n",
    "- Deployment to production\n",
    "- Real-time classification of emails and SMS\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
